# CUDA to Metal Porting Guide

A reference for translating CUDA kernel concepts to Metal via MLX's `mx.fast.metal_kernel()`.

## Thread Indexing

| CUDA | Metal (MLX) | Type | Notes |
|------|-------------|------|-------|
| `threadIdx.x` | `thread_index_in_threadgroup` | `uint` (scalar) | **Not** `.x` — it's already scalar |
| `threadIdx.y` | `thread_index_in_threadgroup / threads_per_threadgroup.x` | | MLX kernels typically use 1D threadgroups, making this unnecessary. With actual 2D threadgroups `(tgx, tgy, 1)`, compute row as `thread_index_in_threadgroup / tgx` and col as `thread_index_in_threadgroup % tgx` since `thread_index_in_threadgroup` is always a scalar `uint`. |
| `blockIdx.x` | `threadgroup_position_in_grid.x` | `uint3` | Use `.x` |
| `blockIdx.y` | `threadgroup_position_in_grid.y` | `uint3` | Use `.y` |
| `blockDim.x` | `threads_per_threadgroup.x` | `uint3` | Use `.x` |
| `gridDim.x` | `threads_per_grid.x / threads_per_threadgroup.x` | | Compute manually |
| `blockIdx.x * blockDim.x + threadIdx.x` | `thread_position_in_grid.x` | `uint3` | Built-in global ID |

### Critical Difference: Grid Size

| | CUDA | MLX Metal |
|-|------|-----------|
| `grid` parameter means | Number of **blocks** | Total number of **threads** |
| Example: 1024 elements, block=256 | `<<<4, 256>>>` | `grid=(1024,1,1), threadgroup=(256,1,1)` |
| Example: N rows, tg=256 | `<<<N, 256>>>` | `grid=(N*256,1,1), threadgroup=(256,1,1)` |

In CUDA, you specify the number of blocks and block size. In MLX, you specify the total thread count and threadgroup size. Metal computes `ceil(grid / threadgroup)` threadgroups.

## Memory Spaces

| CUDA | Metal (MLX) | Notes |
|------|-------------|-------|
| `__global__ void kernel(...)` | Auto-generated by `metal_kernel` | You write only the body |
| `__shared__ float s[N]` | `threadgroup float s[N]` | 32KB limit on Apple Silicon |
| `__constant__` | `constant` | Or use `header="constant float X = ...;"` |
| Global memory (default) | `device` | Auto-generated for inputs/outputs |
| `cudaMalloc` / `cudaMemcpy` | Not needed | Unified memory — CPU/GPU share address space |

### No Host/Device Transfers

In CUDA, you explicitly copy data between host and device memory:
```cpp
// CUDA: explicit transfers
cudaMalloc(&d_x, size);
cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice);
kernel<<<grid, block>>>(d_x, d_out);
cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);
```

In MLX, this is unnecessary — tensors live in unified memory:
```python
# MLX: no transfers needed
x = mx.random.normal((N,))  # Already accessible by GPU
out = kernel(inputs=[x], ...)
mx.eval(out)  # Result is immediately accessible by CPU
```

## Synchronization

| CUDA | Metal (MLX) | Notes |
|------|-------------|-------|
| `__syncthreads()` | `threadgroup_barrier(mem_flags::mem_threadgroup)` | Barrier for threadgroup memory |
| `__threadfence()` | `threadgroup_barrier(mem_flags::mem_device)` | Barrier for device memory |
| `cudaDeviceSynchronize()` | `mx.synchronize()` | Host waits for GPU |
| `cudaStreamSynchronize(s)` | `mx.eval(result)` | Forces evaluation |

**No streams**: MLX manages command scheduling internally. There's no equivalent of CUDA streams or manual kernel queuing.

## Warp/SIMD Operations

| CUDA | Metal (MLX) | Notes |
|------|-------------|-------|
| `warpSize` | 32 | Always 32 on Apple Silicon (hardcode it) |
| `__shfl_xor_sync(mask, val, offset)` | `simd_shuffle_xor(val, offset)` | No mask needed |
| `__shfl_down_sync(mask, val, offset)` | `simd_shuffle_down(val, offset)` | No mask needed |
| `__shfl_up_sync(mask, val, offset)` | `simd_shuffle_up(val, offset)` | No mask needed |
| `__shfl_sync(mask, val, lane)` | `simd_shuffle(val, lane)` | No mask needed |
| Warp-level reduction (manual) | `simd_sum(val)` | Built-in full reduction |
| Warp-level max (manual) | `simd_max(val)` | Built-in full reduction |

**No sync mask**: CUDA's `__shfl_*_sync` requires a mask of active lanes. Metal simdgroup operations don't need masks — all lanes in the simdgroup participate.

## Atomic Operations

| CUDA | Metal (MLX) | Notes |
|------|-------------|-------|
| `atomicAdd(&x, val)` | `atomic_fetch_add_explicit(&x, val, memory_order_relaxed)` | Need `atomic_outputs=True` |
| `atomicMax(&x, val)` | `atomic_fetch_max_explicit(&x, val, memory_order_relaxed)` | |
| `atomicCAS(&x, cmp, val)` | `atomic_compare_exchange_weak_explicit(...)` | |

In MLX, you must set `atomic_outputs=True` when creating the kernel:
```python
kernel = mx.fast.metal_kernel(
    ...,
    source="atomic_fetch_add_explicit((device atomic<float>*)&out[idx], val, memory_order_relaxed);",
    atomic_outputs=True,
)
```

## Math Functions

| CUDA | Metal | Notes |
|------|-------|-------|
| `expf(x)` | `metal::exp(x)` | |
| `logf(x)` | `metal::log(x)` | |
| `sqrtf(x)` | `metal::sqrt(x)` | |
| `rsqrtf(x)` | `metal::rsqrt(x)` | |
| `tanhf(x)` | `metal::tanh(x)` | |
| `fmaxf(a, b)` | `metal::max(a, b)` | Or just `max(a, b)` |
| `fminf(a, b)` | `metal::min(a, b)` | |
| `__expf(x)` (fast) | `metal::fast::exp(x)` | Less precise |
| `fabsf(x)` | `metal::abs(x)` | |

## Porting Example: Vector Add

### CUDA
```cpp
__global__ void vector_add(const float* a, const float* b, float* out, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) out[i] = a[i] + b[i];
}
// Launch: vector_add<<<(N+255)/256, 256>>>(d_a, d_b, d_out, N);
```

### MLX Metal
```python
kernel = mx.fast.metal_kernel(
    name="vector_add",
    input_names=["a", "b"],
    output_names=["out"],
    source="""
    uint i = thread_position_in_grid.x;
    if (i < out_shape[0]) {
        out[i] = (T)(float(a[i]) + float(b[i]));
    }
    """,
)
out = kernel(
    inputs=[a, b],
    template=[("T", mx.float32)],
    grid=(N, 1, 1),
    threadgroup=(256, 1, 1),
    output_shapes=[(N,)],
    output_dtypes=[mx.float32],
)[0]
```

## Key Differences Summary

1. **Unified memory**: No `cudaMalloc`/`cudaMemcpy` — CPU and GPU share memory
2. **Grid = total threads**: Not number of blocks
3. **`thread_index_in_threadgroup` is scalar**: Not a `uint3` — don't use `.x`
4. **No sync masks**: Simdgroup ops don't need active-lane masks
5. **No streams**: MLX manages scheduling; use `mx.eval()` / `mx.synchronize()`
6. **Auto-generated signature**: You write only the kernel body, not the function declaration
7. **Lazy evaluation**: Kernel runs at `mx.eval()` time, not at the call site
